---
title: "BellaBeat_Case_Study_Google_DA_Certificate"
author:
  name: Mohamed Fergany Omran
  email: mohamed.fergany.omran@gmail.com
date: "3/19/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE
) 
```

```{r}
# Before we begin, load all the required libraries
library(tidyverse)
library(lubridate)
library(janitor)
library(here)
library(skimr)
library(scales)
```

# Introduction

> The Chief executive officer of BellaBeat has asked you to focus on a Bellabeat product and analyze smart device usage data in order to gain insight into how consumers use non-Bellabeat smart devices. She then wants you to select one Bellabeat product to apply these insights to in your presentation.

Based on the provided information above, our business task is now clear. Use usage data from non-Bellabeat yet similar product. In our case we will use data from Fitbit consumers which is an american consumer electronics and fitness company owned by Google. It's considered to be one of the largest companies in that market. According to [Wikipedia](https://en.wikipedia.org/wiki/Fitbit#cite_note-Keyword-5), Fitbit has more than 29 million active users in their community and has sold more than 120 million devices. Their products are sold in 39,000 retail stores and in over 100 countries. Fitbit has a revenue of [\$1.13 billion](https://www.businessofapps.com/data/fitbit-statistics/) in 2020. Thus choosing data from such a company to inform the marketing strategy for Bellabeat products was a really good decision.

# Ask

Our main business task is to use these data to gain insights about:

1.  Why consumers selected that company to get their smart devices from

2.  What information those devices provide to them

3.  How can Bellabeat use these data to improve their marketing strategy or even improve their product features

# Prepare

For this project we shall use the [Fitbit](https://www.kaggle.com/arashnic/fitbit) dataset from Kaggle. You may wanna take a look at the [metadata](https://www.kaggle.com/arashnic/fitbit/discussion/281341) for easier interpretation. Also have a quick [look](https://www.fitbit.com/global/us/products/trackers/charge5) to know how the tracker actually works. Now According to the description,

> This dataset was generated by respondents to a distributed survey via Amazon Mechanical Turk between 03.12.2016 - 05.12.2016. Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring.

It consists of 18 csv files with data such as dates, times, duration of workouts, amounts of burned calories, number of sleeping hours and body weight info. As mentioned previously, this dataset can be downloaded from Kaggle and I saved a backup copy on my local device. Some data files are stored in wide format and some are stored in the long one. Some files are even stored in both formats.

*It's really important to note here that the CEO, who is our main stakeholder in this case, says that this data set might have some limitations and encourages us to consider adding other data to help address those limitations as we begin to work more with this data.*

1.  Regarding the reliability of the source it's important to mention that Kaggle doesn't put strict conditions to ensure the datasets being uploaded meet specific criteria. Personally I've uploaded multiple datasets to Kaggle without having to answer any questions about the its original source. The dataset uploader howerver states that the source is [Zenodo](https://zenodo.org/) website which in turn states that the owner of the dataset is [RTI](https://www.rti.org/), an independent, nonprofit institute that provides research, development, and technical services to government and commercial clients worldwide. Hence we may say that the source of this datasets is reliable and the dataset is cited.
2.  We haven't collected this dataset by ourselves thus it's a second party data
3.  This dataset was collected and uploaded about six years ago and later in this analysis we shall explore the dataset and decide whether it contains all the information we need or we have to look for other sources
4.  Despite Fitibit tracker is a product that's being sold and used worldwide, This data is collected via Amazon Mechanical Turk where participants from only specific countries are eligible to take the survey and this raise concerns regarding the sampling bias. Other than that, I can't see any other type of bias exist in the data
5.  The dataset owners made it an open access type. It's available for download, copy and use by anyone for free
6.  Regarding ownership, according to the dataset owners thirty eligible Fitbit users consented to the submission of personal tracker data. However no additional information is provided with the description related to the transaction transparency. Thus we don't know whether the participants were aware how exactly their data will processed or not
7.  IDs were used instead of participants names. Also no personally identifiable information (location, contact info, medical records, etc.) are found in the files
8.  In addition to the sampling bias, Compared to the number of customers, the size of the sample is quite small. These two are the main problems associated with this dataset which appeared so far

# Process

We shall use R in both RStudio and Kaggle for this case study because I see R as a more powerful tool than spreadsheets and maybe later we repeat the case study with SQL and compare between the two tools. The dataset has 18 csv files so let's explore and clean the dataset step by step.

**1) Daily Activity**

```{r}
# Step_1
# First modify the dataframe columns names to a more standard format
dailyActivity_merged = read_csv(
  "case_study/Fitabase Data 4.12.16-5.12.16/dailyActivity_merged.csv"
)

dailyActivity_merged = clean_names(dailyActivity_merged)

# Running the below code, we can see we have 33 id/person
dailyActivity_merged %>%
  count(id)
```

As you can see we have total number of IDs equals 33. But as mentioned earlier we should only have thirty ids. So could it Possible that there are some users with more than one device?!! Well as you can see below for some days we have the whole 33 participants working out on the same day, thus we shall consider we have 33 participants until we got a strong valid evidence that proves otherwise. Another important note is that despite the description says that the duration of study is between 3/12/2016 & 5/12/2016 (62 days). The first day in the dataset is 4/12/2016 thus the total number of days in the analysis 31 days.

```{r}
dailyActivity_merged %>%
  group_by(activity_date) %>%
  summarize(no_of_participants = n_distinct(id))
```

```{r}
# Step_2
# Remove any duplicate entries if exist
dailyActivity_merged = unique(dailyActivity_merged)
```

```{r}
# Step_3
# Check the data types of each column and modify if necessary
str(dailyActivity_merged)
dailyActivity_merged$activity_date = mdy(dailyActivity_merged$activity_date)
```

```{r}
# Step_4
# Take a quick overview the dataframe and check for any messing values
skim_without_charts(dailyActivity_merged)
```

But maybe despite not having any null values we still have rows that all of its values equal zero so let's check.

```{r}
# Step_5
# Check for any rows where every column has zero value
filter(dailyActivity_merged, if_all(everything(), ~ . == 0))
```

```{r}
# Step_6
# Check for days where there are no input data
input_data = dplyr::select(dailyActivity_merged, !c("id", "activity_date"))
filter(input_data, if_all(everything(), ~ . == 0))
```

```{r}
# Step_7
# Check if there're no IDs entered or entered inocrrectly (more or less than 
# 10 numbers)
min(dailyActivity_merged$id)
max(dailyActivity_merged$id)
```

*From the following code, we can see that the value of tracker distance equals the total distance which is the sum of four degree of active distance ONLY when there isn't any value entered for the logged activities. However we shall not consider that as a cross-field invalidation and leave the dataframe as it is for now.*

```{r}
dailyActivity_merged %>%
  filter(
   tracker_distance == rowSums(dailyActivity_merged[, 7:10]) &
     total_distance == tracker_distance & logged_activities_distance == 0
  )
```

*The dataframes daily calories, daily intensities and daily steps are just a replication of the data in daily activity one. Thus they shall not be considered in this analysis. In general out of the 18 dataframes we're going to use only six of them as the other 12 have replicate / unnecessary info. That said let's process to clean the remaining five.*

**2) Burned Calories per Hour**

```{r}
# Step_1
# First modify the dataframe columns names to a more standard format
hourlyCalories_merged = read_csv(
  "case_study/Fitabase Data 4.12.16-5.12.16/hourlyCalories_merged.csv"
)
hourlyCalories_merged = clean_names(hourlyCalories_merged)

# Step_2
# Remove any duplicate entries if exist
hourlyCalories_merged = unique(hourlyCalories_merged)

# Step_3
# Check the data types of each column and modify if necessary
str(hourlyCalories_merged)
hourlyCalories_merged$activity_hour = mdy_hms(hourlyCalories_merged$activity_hour)

# Step_4
# Take a quick overview the dataframe and check for any messing values
# This is also useful to make sure all dates have been converted correctly. 
# Otherwise. we'll end up having null values
skim_without_charts(hourlyCalories_merged)

# Step_5
# Check for any zero values
filter(hourlyCalories_merged, if_any(everything(), ~ . == 0))

# Step_6
# Check if there're no IDs entered or entered inocrrectly (more or less than 
# 10 numbers)
min(hourlyCalories_merged$id)
max(hourlyCalories_merged$id)

# Step_7
# Check for any wrong calories values
hourlyCalories_merged %>%
  filter(calories < 0)
```

**3) Intensities Values per Hour**

```{r}
# Step_1
# First modify the dataframe columns names to a more standard format
hourlyIntensities_merged = read_csv(
  "case_study/Fitabase Data 4.12.16-5.12.16/hourlyIntensities_merged.csv"
)
hourlyIntensities_merged = clean_names(hourlyIntensities_merged)

# Step_2
# Remove any duplicate entries if exist
hourlyIntensities_merged = unique(hourlyIntensities_merged)

# Step_3
# Check the data types of each column and modify if necessary
str(hourlyIntensities_merged)
hourlyIntensities_merged$activity_hour = mdy_hms(hourlyIntensities_merged$activity_hour)

# Step_4
# Take a quick overview the dataframe and check for any messing values
# This is also useful to make sure all dates have been converted correctly. 
# Otherwise. we'll end up having null values
skim_without_charts(hourlyIntensities_merged)

# Step_5
# Check for any row that has all zero values
filter(hourlyIntensities_merged, if_all(everything(), ~ . == 0))

# Step_6
# Check if there're no IDs entered or entered inocrrectly (more or less than 
# 10 numbers)
min(hourlyIntensities_merged$id)
max(hourlyIntensities_merged$id)

# Step_7
# Check for any wrong intensities values
hourlyIntensities_merged %>%
  filter(total_intensity < 0 | average_intensity < 0)
```

**4) Steps per Hour**

```{r}
# Step_1
# First modify the dataframe columns names to a more standard format
hourlySteps_merged = read_csv(
  "case_study/Fitabase Data 4.12.16-5.12.16/hourlySteps_merged.csv"
)
hourlySteps_merged = clean_names(hourlySteps_merged)

# Step_2
# Remove any duplicate entries if exist
hourlySteps_merged = unique(hourlySteps_merged)

# Step_3
# Check the data types of each column and modify if necessary
str(hourlySteps_merged)
hourlySteps_merged$activity_hour = mdy_hms(hourlySteps_merged$activity_hour)

# Step_4
# Take a quick overview the dataframe and check for any messing values
# This is also useful to make sure all dates have been converted correctly. 
# Otherwise. we'll end up having null values
skim_without_charts(hourlySteps_merged)

# Step_5
# Check for any row that has all zero values
filter(hourlySteps_merged, if_all(everything(), ~ . == 0))

# Step_6
# Check if there're no IDs entered or entered inocrrectly (more or less than 
# 10 numbers)
min(hourlySteps_merged$id)
max(hourlySteps_merged$id)

# Step_7
# Check for any wrong steps values
hourlySteps_merged %>%
  filter(step_total < 0)
```

**5) Sleep Periods**

```{r}
# Step_1
# First modify the dataframe columns names to a more standard format
sleepDay_merged = read_csv(
  "case_study/Fitabase Data 4.12.16-5.12.16/sleepDay_merged.csv"
)
sleepDay_merged = clean_names(sleepDay_merged)

# Step_2
# Remove any duplicate entries if exist
sleepDay_merged = unique(sleepDay_merged)

# Step_3
# Check the data types of each column and modify if necessary
str(sleepDay_merged)
sleepDay_merged$sleep_day = mdy_hms(sleepDay_merged$sleep_day)

# Step_4
# Take a quick overview the dataframe and check for any messing values
# This is also useful to make sure all dates have been converted correctly.   
# Otherwise. we'll end up having null values
skim_without_charts(sleepDay_merged)

# Step_5
# Check for any zero values
filter(sleepDay_merged, if_all(everything(), ~ . == 0))

# Step_6
# Check if there're no IDs entered or entered inocrrectly (more or less than 
# 10 numbers)
min(sleepDay_merged$id)
max(sleepDay_merged$id)

# Step_7
# Check for any wrong values
sleepDay_merged %>%
  filter(
    total_sleep_records < 0 | total_minutes_asleep < 0 | total_time_in_bed < 0
  )
```

**6) Body Mass Info**

```{r}
# Step_1
# First modify the dataframe columns names to a more standard format
weightLogInfo_merged = read_csv(
  "case_study/Fitabase Data 4.12.16-5.12.16/weightLogInfo_merged.csv"
)
weightLogInfo_merged = clean_names(weightLogInfo_merged)

# Step_2
# Remove any duplicate entries if exist
weightLogInfo_merged = unique(weightLogInfo_merged)

# Step_3
# Check the data types of each column and modify if necessary
str(weightLogInfo_merged)
weightLogInfo_merged$date = mdy_hms(weightLogInfo_merged$date)

# Step_4
# Take a quick overview the dataframe and check for any messing values
# This is also useful to make sure all dates have been converted correctly. Otherwise
# we'll end up having null values
skim_without_charts(weightLogInfo_merged)

# Step_5
# Check for any zero values
filter(weightLogInfo_merged, if_all(everything(), ~ . == 0))

# Step_6
# Check if there're no IDs entered or entered inocrrectly (more or less than 10 numbers)
min(weightLogInfo_merged$id)
max(weightLogInfo_merged$id)
min(weightLogInfo_merged$log_id)
max(weightLogInfo_merged$log_id)

# Step_7
# Check for any wrong values
# We know from step_4 that the fat column has too many null values 
weightLogInfo_merged %>%
  drop_na(fat) %>%
  filter(
    weight_kg < 0 | weight_pounds < 0 | fat < 0 | bmi < 0 | 
      is_manual_report == 0
  )
```

# Analyze & Share

```{r}
# Step_1 summarize the following dataframes
merged_1 = merge(
  hourlySteps_merged,
  hourlyIntensities_merged,
  by = c("id", "activity_hour")
)

hourly_sic_merged = merge(
  merged_1,
  hourlyCalories_merged,
  by = c("id", "activity_hour")
)

head(hourly_sic_merged)
```

```{r}
# To see the total number of sumbitted hours per each participant
hour_frequency = tabyl(hourly_sic_merged, id) %>%
  arrange(-n) %>%
  dplyr::rename(no_of_submitted_hours = n) %>%
  dplyr::select(-percent)

hour_frequency
```

```{r}
daily_frequency = tabyl(dailyActivity_merged, id) %>%
  arrange(-n) %>%  
  dplyr::rename(no_of_days = n) %>%
  dplyr::select(-percent)

daily_frequency
```

```{r}
merged_frequencies = merge(hour_frequency, daily_frequency, by = "id") 

merged_frequencies = merged_frequencies %>%
  arrange(-no_of_submitted_hours)

merged_frequencies
```

```{r}
summarized_hourly_data = hourly_sic_merged %>%
  group_by(id) %>%
  summarize(
    avg_calories_per_hour = mean(calories),
    avg_intensities_per_hour = mean(total_intensity),
    avg_steps_per_hour = mean(step_total)
  )

summarized_hourly_data
```

```{r}
weight_summary = weightLogInfo_merged %>%
  group_by(id) %>%
  summarize(
    avg_weight_kg = mean(weight_kg),
    avg_BMI = mean(bmi)
  )

joined_df = full_join(merged_frequencies, weight_summary)

summary_of_daily_data = dailyActivity_merged %>%
  group_by(id) %>%
  summarise(
    avg_steps_per_day = mean(total_steps),
    avg_distance_per_day = mean(total_distance),
    avg_calories_per_day = mean(calories),
  )

summary_data_1 = full_join(joined_df, summary_of_daily_data)
summary_data = full_join(summary_data_1, summarized_hourly_data)

summary_data
```

```{r}
# Now let's find out if there's a relation between the weekday and workouts 
daily_activity_wday = dailyActivity_merged %>%
  mutate("week_day" = weekdays(activity_date), .after = activity_date)

ggplot(
  daily_activity_wday, 
  aes(week_day, fill = week_day)
) + 
  geom_bar(show.legend = FALSE) + 
  labs(
    title = "Participation vs Weekday",
    subtitle = "The relation between the weekday and the frequency of workouts",
    caption = "Data colllected by the Research Triangle Institute"
  ) + 
  theme(
    axis.text = element_text(size = 10, angle = 45),
    axis.text.x = element_text(vjust = 0.7),
    axis.title = element_text(size = 15)
  ) +
  stat_count(
    geom = 'text', 
    color = 'black', 
    aes(label = ..count..), 
    size = 5, 
    position = position_stack(vjust = 0.5)
  ) 
```

*As we can see there's a weak relation between the weekday and the frequency of participation. Now let's see if there's a relation between the weekday and the amount of total distance as that refelcts the amount of physical activity . Also if there's a relation between the time of day and workouts*

```{r}
daily_activity_wday %>%
  group_by(week_day) %>%
  summarise(total_distance = sum(total_distance)) %>%
  ggplot(aes(week_day, total_distance, fill = week_day)) + 
  geom_col(show.legend = FALSE) + 
  geom_text(
    aes(label = round(total_distance, 0)), 
    position=position_stack(vjust = 0.5)
  ) + 
  theme(
    axis.text = element_text(size = 10, angle = 45),
    axis.text.x = element_text(vjust = 0.7),
    axis.title = element_text(size = 15)
  ) + 
  labs(
    title = "Total_distance vs Weekday",
    subtitle = "The relation between the weekday and the amount of total            distance",
    caption = "Data collected by the Research Triangle Institute",
    x = "Weekday",
    y = "Total Distance (Km)"
  ) 
```

## Finding No.1

As we can see from the above chart, participants tend to practice less at the beginning and end of the week (Friday and Monday) and also on weekends. It's really interesting that despite having more spare time on weekends, they workout more on working days than the non working ones. Also on Monday after returning to work, we can see there's some kind of a humble start and that's understandable from personal experience as the first working day is a sort of transitional phase between two different daily routines. what really surprised me however is Friday, is it because the participants after a long week preferred to spend their Friday nights on something else other than workouts? Let's check the below charts to find out more.

```{r include=FALSE}
daily_activity_wday %>%
  group_by(week_day) %>%
  summarise(avg_amt_of_calories = mean(calories)) %>%
  ggplot(aes(week_day, avg_amt_of_calories, fill = week_day)) + 
  geom_col(show.legend = FALSE) + 
  geom_text(
    aes(label = round(avg_amt_of_calories, 0)), 
    position=position_stack(vjust = 0.5)
  ) + 
  theme(
    axis.text = element_text(size = 10, angle = 45),
    axis.text.x = element_text(vjust = 0.7),
    axis.title = element_text(size = 15)
  ) + 
  labs(
    title = "Avg amount of calories vs Weekday",
    subtitle = "The relation between the weekday and the avg amount of calories"     , caption = "Data collected by the Research Triangle Institute"
  ) 
```

```{r}
hourly_sic_merged = hourly_sic_merged %>%
  mutate(hour = hour(activity_hour), .after = activity_hour)

average_values = hourly_sic_merged %>%
  group_by(hour) %>%
  summarize(
    avg_amt_of_calories = mean(calories),
    avg_total_steps = mean(step_total),   
    avg_intensity_per_hour = mean(total_intensity)
  )

ggplot(average_values, aes(hour, avg_amt_of_calories, fill = hour)) + 
  geom_col(show.legend = FALSE) +
  geom_text(
    aes(label = round(avg_amt_of_calories,0)), 
    size = 3.5, 
    color = 'white',
    angle = 90,
    position = position_stack(vjust = 0.5) 
  ) + 
  scale_x_continuous(n.breaks = 23) + 
  labs(
    title = "Avg amount of calories vs hour",
    subtitle = "The relation between the time of the day and the avg amount of calories",
    caption = "Data collected by the Research Triangle Institute",
    x = "Hour",
    y = "Average Bunred Calories"
  ) 
```

```{r}
ggplot(average_values, aes(hour, avg_intensity_per_hour, fill = hour)) +
  geom_col(show.legend = FALSE) +
  scale_x_continuous(n.breaks = 23) +
  geom_label(
    aes(label = round(avg_intensity_per_hour, 0)),
    fill = "white"
  ) +
  labs(
    title = "Avg amount of calories vs hour",
    subtitle = "The relation between the time of the day and the avg amount of calories" ,   caption = "Data collected by the Research Triangle Institute",
    x = "Hour",
    y = "Average Intensity per Hour"
  )
```

## Finding No.2

By looking at the above charts we can clearly see the relation between the time of day and the degree of physical activity. Knowing that the usual working hours are from 8 to 5, we may focus on encouraging customers to workout during these periods (6:00 AM to 8:00 AM) and (8:00 PM to 11:00 PM) while maintaining seven sleeping hours per day.

```{r}
sleepDay_merged = sleepDay_merged %>%
  mutate(week_day = weekdays(sleep_day), .after = sleep_day)
head(sleepDay_merged)
```

```{r}
sleepDay_merged %>%
  group_by(week_day) %>%
  summarise(average_sleeping_hours = mean(total_minutes_asleep)/60) %>%
  ggplot(aes(week_day, average_sleeping_hours, fill = week_day)) + 
  geom_col(show.legend = FALSE) + 
  geom_text(
    aes(label = round(average_sleeping_hours, 1)), 
    position=position_stack(vjust = 0.5)
  ) + 
  theme(
    axis.text = element_text(size = 10, angle = 45),
    axis.text.x = element_text(vjust = 0.7),
    axis.title = element_text(size = 15)
  ) + 
  labs(
    title = "Average Sleeping Hours vs Weekday",
    subtitle = "The relation between the weekday and the avg amount of sleeping hours",
    caption = "Data collected by the Research Triangle Institute",
    x = "Weekday",
    y = "Average Hours Asleep"
  ) 
```

```{r}
sleepDay_merged %>%
  group_by(week_day) %>%
  summarise(average_time_in_bed = mean(total_time_in_bed)/60) %>%
  ggplot(aes(week_day, average_time_in_bed, fill = week_day)) + 
  geom_col(show.legend = FALSE) + 
  geom_text(
    aes(label = round(average_time_in_bed, 1)), 
    position=position_stack(vjust = 0.5)
  ) + 
  theme(
    axis.text = element_text(size = 10, angle = 45),
    axis.text.x = element_text(vjust = 0.7),
    axis.title = element_text(size = 15)
  ) + 
  labs(
    title = "Average Time in Bed vs Weekday",
    subtitle = "The relation between the weekday and the average time in bed",
    caption = "Data collected by the Research Triangle Institute",
    x = "Weekday",
    y = "Average Hours in Bed"
  ) 
```

```{r}
ggplot(
  sleepDay_merged, 
  aes(x = factor(total_sleep_records), fill = total_sleep_records)
) + 
  geom_bar()  +
  labs(
    title = "Count of Sleep Records",
    subtitle = "Comparison between the Sleep Records ",
    caption = "Data collected by the Research Triangle Institute",
    x = "Total Sleep Records",
    y = "Count"
  ) 
```

## Finding No.3

The recommended amount of sleeping hours for adults is (7-9) hours per day. Thus we shouldn't fall below the minimum of seven hours to maintain a healthy daily routine. By looking at the above graphs, we can see that the majority don't have the ability and/or the willingness to get a \>60 minutes nap during the day. Although they always devote more than 7 hours for bed time each day, the actual sleeping hours is often less than 7.

```{r}
# We can see that not all 33 participants have shared their weight, sleep periods
percent(
  (n_distinct(dailyActivity_merged$id) - n_distinct(sleepDay_merged$id)) / 33
)
```

## Finding No.4

Another important observation from this dataframe is that more than a quarter of the participants haven't shared any data about their sleeping periods. that's a percentage that needs to be taken into consideration. we need to find the reason(s) why they're not interested to use the fitbit tracker to record their sleep quality (Awake/Light/REM/Deep) while being in bed.

```{r}
n_distinct(weightLogInfo_merged$id)

weightLogInfo_merged %>%
  drop_na(fat) %>%
  summarise(count = n_distinct(id))
```

## Finding No.5

We can see that only 8 out of 33 participants have shared their weight & BMI either using a connected smart scale or manually. We also see that almost no one of the participants have shared his/her body fat percentage.

# Act

*The five recommendations shown below are based on the five findings in our dataset respectively. Before proceeding allow me to remind you what I've said earlier about the limitations of this dataset and why we should collect more data by conducting another survey that's more global, sepcifically addressed for women and has more participants. The CEO has also asked to do so.*

1.  It's finally the weekend and everyone of us wants to hang out with some friends, binge watch some shows, go for a quick trip and the list goes on. For me I can see the challenge here is not just to send a reminder via the app for users not to miss training on those days but rather to prepare some fun collections of different workouts that are mainly bodyweight ones, so they can be done anywhere. Thus no matter where you're planning to go on this weekend you don't need to go look for a gym or a good running track and you don't even to carry those exercise tools with you. I think what we need to bring to the table is a diverse list of fun bodyweight programs and keep updating those programs so that users are looking forward to explore them each new weekend. we may also bring group workouts programs to encourage friends to do them while hanging out.

2.  There are many fitness experts that strongly advise to workout in the early morning to get the best results for body. You may wanna read this [article](https://time.com/5533388/best-time-to-exercise/) by the TIME for instance. We may develop collection of morning workouts. To keep our users motivated we may send daily notifications such as links to articles, studies, etc. that are showing more information on the benefits of waking up early and practice before going to work. This will also help the users to not miss workouts on Fridays.

3.  Sending a quick info & charts to users about their sleep quality last night and how that will affect the day ahead can be a good start. Also a *creative* reminders that are set at a user_defined times each day to tell the user to go to bed and turn off any distractions

4.  & 5 We need to conduct another survey for Fitbit users asking questions about

    a\. Whether they use their tracker while they're asleep or not

    b\. Is the sleep-related information they receive from the tracker helpful or not

    c\. Asking whether the tracker itself is comfortable to wear while asleep or not and if not why it's uncomfortable

    d\. Do they record their weight, BMI and fat info or not

    e\. Is the related information they receive from the tracker helpful or not

    f\. On average how long does it take to record this weight-related info via the app

    p.s. *I know the following two questions are out of scope of this case study but since we're conducting a survey anyway let's gather this info it for future considerations*

    g\. If they're planning to get a new smart scale what physical and technical features are they looking for and what's the desired price range

    h\. why they've chosen this brand to get their current tracker from

    i\. If they're planning to get a new fitness tracker what physical and technical features are they looking for and what's the desired price range

    j\. Would they use their trackers to look for dietary plans and why

    k\. Finally we may request feedback and suggestions. For instance when they're looking for information about health status, sleep quality, etc. what they're expecting
